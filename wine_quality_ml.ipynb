{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38588c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries 169 lines of code and 256 including the comments and unecessary spacing \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import joblib\n",
    "from statistics import mode\n",
    "import os  # For file handling\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the dataset from the provided file name\n",
    "data = pd.read_csv('wine quality data set 2000.csv')\n",
    "\n",
    "# Check basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\nFirst five rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Binarize the target into 3 categories: 5->'Bad', 6->'Average', 7+->'Good'\n",
    "data['quality'] = data['quality'].apply(lambda x: 'Good' if x >= 7 else 'Average' if x == 6 else 'Bad')\n",
    "\n",
    "# Visualize the dataset with custom colors\n",
    "sns.countplot(x='quality', data=data, palette=\"mako\")\n",
    "plt.title('Distribution of Wine Quality')\n",
    "plt.show()\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Use SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "# Visualize the distribution after SMOTE\n",
    "sns.countplot(x=y, palette=\"mako\")\n",
    "plt.title('Distribution of Wine Quality After SMOTE')\n",
    "plt.show()\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check if we already have pre-trained models and load them\n",
    "rf_model_path = 'best_rf_model.pkl'\n",
    "gb_model_path = 'best_gb_model.pkl'\n",
    "if os.path.exists(rf_model_path) and os.path.exists(gb_model_path):\n",
    "    best_rf_model = joblib.load(rf_model_path)\n",
    "    best_gb_model = joblib.load(gb_model_path)\n",
    "    print(\"Loaded pre-trained models.\")\n",
    "else:\n",
    "    # Initialize models\n",
    "    best_rf_model = RandomForestClassifier(random_state=42)\n",
    "    best_gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Feature Selection using Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "selector = RFE(rf, n_features_to_select=5)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X.columns[selector.support_]\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for better cross-validation\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Grid search for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=best_rf_model, param_grid=param_grid_rf, cv=cv, verbose=2, n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_selected, y_train)\n",
    "print(f\"Best Random Forest Parameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "# Grid search for Gradient Boosting\n",
    "grid_search_gb = GridSearchCV(estimator=best_gb_model, param_grid=param_grid_gb, cv=cv, verbose=2, n_jobs=-1)\n",
    "grid_search_gb.fit(X_train_selected, y_train)\n",
    "print(f\"Best Gradient Boosting Parameters: {grid_search_gb.best_params_}\")\n",
    "\n",
    "# Train the models with the best parameters\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "\n",
    "# Combine predictions using weighted probabilities\n",
    "# Assign weights to the models (you can tune these weights)\n",
    "weight_rf = 0.5\n",
    "weight_gb = 0.5\n",
    "\n",
    "print(\"\\nCombining predictions using weighted probabilities...\")\n",
    "print(f\"Random Forest Weight: {weight_rf}\")\n",
    "print(f\"Gradient Boosting Weight: {weight_gb}\")\n",
    "\n",
    "# Get probabilities for each model\n",
    "prob_rf = best_rf_model.predict_proba(X_test_selected)\n",
    "prob_gb = best_gb_model.predict_proba(X_test_selected)\n",
    "\n",
    "# Calculate combined probabilities\n",
    "prob_combined = (weight_rf * prob_rf) + (weight_gb * prob_gb)\n",
    "\n",
    "# Get final predictions based on the highest combined probability\n",
    "y_pred_combined = [best_rf_model.classes_[np.argmax(prob)] for prob in prob_combined]\n",
    "\n",
    "# Output the accuracy of the combined model\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined)\n",
    "print(f\"\\nCombined Model Accuracy on test data: {accuracy_combined * 100:.2f}%\")\n",
    "\n",
    "# Display the confusion matrix for the combined model\n",
    "print(\"\\nCombined Model Confusion Matrix:\")\n",
    "cm_combined = confusion_matrix(y_test, y_pred_combined)\n",
    "sns.heatmap(cm_combined, annot=True, fmt='d', cmap='Purples', xticklabels=['Bad', 'Average', 'Good'], yticklabels=['Bad', 'Average', 'Good'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Combined Model')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for the combined model\n",
    "print(\"\\nCombined Model Classification Report:\")\n",
    "report_combined = classification_report(y_test, y_pred_combined)\n",
    "print(report_combined)\n",
    "\n",
    "# Save the models\n",
    "joblib.dump(best_rf_model, rf_model_path)\n",
    "joblib.dump(best_gb_model, gb_model_path)\n",
    "print(\"Models saved to 'best_rf_model.pkl' and 'best_gb_model.pkl'\")\n",
    "\n",
    "# Define valid input ranges for features\n",
    "valid_ranges = {\n",
    "     'ethanol': (8.0, 15.0),\n",
    "    'pH': (2.5, 4.5),\n",
    "    'acid': (0.1, 1.5),\n",
    "    'sul': (1.0, 300.0),  # assuming 'sul' as a general sulfur dioxide-related feature\n",
    "    'sugar': (0.5, 15.0),\n",
    "    'phenolics': (0.1, 5.0),  # assuming general phenolics content range\n",
    "    'anthocyanin': (0.0, 1.0),\n",
    "    'ethyl_acetate': (0.01, 1.0),\n",
    "    'ethyl_2_methylpropanoate': (0.01, 1.0),\n",
    "    'ethyl_butanoate': (0.01, 1.0),\n",
    "    # Adding remaining features in the dataset with reasonable assumed ranges\n",
    "    'ethyl_3_methylbutanoate': (0.01, 1.0),\n",
    "    'methylpropan_1_ol': (0.01, 1.0),\n",
    "    'methylbutyl_acetate': (0.01, 1.0),\n",
    "    'ethyl_pentanoate': (0.01, 1.0),\n",
    "    'methylbutan_1_ol': (0.01, 1.0),\n",
    "    'ethyl_hexanoate': (0.01, 1.0),\n",
    "    'hexyl_acetate': (0.01, 1.0),\n",
    "    'ethyl_2_hydroxypropanoate': (0.01, 1.0),\n",
    "    'hexan_1_ol': (0.01, 1.0),\n",
    "    'hex_3_en_1_ol': (0.01, 1.0),\n",
    "    'heptan_1_ol': (0.01, 1.0),\n",
    "    'ethyl_octanoate': (0.01, 1.0),\n",
    "    'benzaldehyde': (0.01, 0.1),\n",
    "    'ethyl_decanoate': (0.01, 1.0),\n",
    "    'phenol': (0.01, 0.1),\n",
    "    '2_methoxyphenol': (0.01, 0.1),\n",
    "    'ethyl_3_phenylpropanoate': (0.01, 1.0),\n",
    "    'acetic_acid': (0.01, 1.0),\n",
    "    'butanoic_acid': (0.01, 1.0),\n",
    "    'octanoic_acid': (0.01, 1.0),\n",
    "    'hexanoic_acid': (0.01, 1.0),\n",
    "}\n",
    "\n",
    "# Now, allow the user to input new values for prediction\n",
    "print(\"\\n--- Enter details for a single wine sample ---\")\n",
    "\n",
    "# Define a function to get valid input from the user\n",
    "def get_float_input(prompt, feature):\n",
    "    while True:\n",
    "        try:\n",
    "            value = float(input(prompt))\n",
    "            if valid_ranges[feature][0] <= value <= valid_ranges[feature][1]:\n",
    "                return value\n",
    "            else:\n",
    "                print(f\"Invalid input. Please enter a value between {valid_ranges[feature][0]} and {valid_ranges[feature][1]}.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a numeric value.\")\n",
    "\n",
    "# Dictionary to hold user inputs\n",
    "user_input = {}\n",
    "\n",
    "# Prompt the user for the selected features only\n",
    "for feature in selected_features:\n",
    "    user_input[feature] = get_float_input(f\"Enter {feature}: \", feature)\n",
    "\n",
    "# Create a DataFrame for the input values with all features\n",
    "full_input = pd.DataFrame(columns=X.columns, index=[0])\n",
    "\n",
    "# Fill in the values for the selected features\n",
    "for feature in selected_features:\n",
    "    full_input.loc[0, feature] = user_input[feature]\n",
    "\n",
    "# Fill in the remaining features with zero\n",
    "for feature in X.columns:\n",
    "    if feature not in selected_features:\n",
    "        full_input.loc[0, feature] = 0\n",
    "\n",
    "# Ensure the input has the right shape for scaling\n",
    "single_input_scaled = scaler.transform(full_input)\n",
    "\n",
    "# Use the selector to transform the scaled input\n",
    "single_input_selected = selector.transform(single_input_scaled)\n",
    "\n",
    "# Make predictions and get probabilities using the best models\n",
    "single_prob_rf = best_rf_model.predict_proba(single_input_selected)[0]\n",
    "single_prob_gb = best_gb_model.predict_proba(single_input_selected)[0]\n",
    "\n",
    "# Combine probabilities using the assigned weights\n",
    "single_prob_combined = (weight_rf * single_prob_rf) + (weight_gb * single_prob_gb)\n",
    "\n",
    "# Determine the final prediction based on the highest combined probability\n",
    "single_prediction_combined = best_rf_model.classes_[np.argmax(single_prob_combined)]\n",
    "\n",
    "# Print probabilities for each class as percentages in an organized format\n",
    "print(\"\\nPrediction Probabilities (as percentages):\")\n",
    "print(f\"{'Quality':<10} {'Random Forest':>15} {'Gradient Boosting':>20} {'Combined':>15}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label, p_rf, p_gb, p_comb in zip(best_rf_model.classes_, single_prob_rf, single_prob_gb, single_prob_combined):\n",
    "    print(f\"{label:<10} {p_rf * 100:>15.2f}% {p_gb * 100:>20.2f}% {p_comb * 100:>15.2f}%\")\n",
    "\n",
    "# Output the predicted wine quality\n",
    "print(f\"\\nPredicted Wine Quality (Combined): {single_prediction_combined}\")\n",
    "\n",
    "# Provide feedback based on the quality score\n",
    "if single_prediction_combined == 'Good':\n",
    "    print(\"This wine is of good quality! A great choice!\")\n",
    "elif single_prediction_combined == 'Average':\n",
    "    print(\"This wine is of average quality. It might be worth trying!\")\n",
    "else:\n",
    "    print(\"This wine is of below-average quality. You might want toÂ reconsider!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
